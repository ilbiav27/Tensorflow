{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HSJung\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import random\n",
    "\n",
    "from datetime import datetime\n",
    "import math\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "import vgg16_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "image_num = 724 #The number of image data\n",
    "eval_num = 196\n",
    "batch_num_t = math.ceil(image_num/batch_size)\n",
    "batch_num_e = math.ceil(eval_num/batch_size)\n",
    "\n",
    "epoch_num = 100\n",
    "classes_num = 21\n",
    "MOVING_AVERAGE_DECAY = 0.9999\n",
    "image_WH = 256\n",
    "\n",
    "loss_ = np.zeros(100)\n",
    "\n",
    "ckpt_loc = 'checkpoint'\n",
    "eval_loc = r'\\\\cat\\share\\정한슬\\cat_data\\data\\eval'\n",
    "data_loc = r'\\\\cat\\share\\정한슬\\cat_data\\data\\train'\n",
    "text = r'\\\\cat\\share\\정한슬\\cat_data\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(ckpt_loc):\n",
    "    os.mkdir(ckpt_loc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image :724\n"
     ]
    }
   ],
   "source": [
    "#make training data vector\n",
    "with open(os.path.join(text,'training.txt'),'r') as f:\n",
    "    #suffle images for training\n",
    "    lines = random.sample(f.readlines(),image_num)\n",
    "image = np.ndarray([image_num,image_WH,image_WH,3])\n",
    "label = np.ndarray([image_num])\n",
    "\n",
    "i = 0\n",
    "for line in lines:\n",
    "    if i == image_num: break\n",
    "    #print(cv2.imread(line[:-4], CV_LOAD_IMAGE_COLOR))\n",
    "    label[i] = np.asanyarray(int(line[-3:-1])).reshape([1])\n",
    "    image[i] = np.asanyarray(Image.open(line[:-4])).reshape((image_WH,image_WH,3))\n",
    "    i += 1\n",
    "print(\"train image :\"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eval image : 196\n"
     ]
    }
   ],
   "source": [
    "#make validation data vector\n",
    "with open(os.path.join(text,'evaluating.txt'),'r') as f:\n",
    "    lines = random.sample(f.readlines(),eval_num)\n",
    "    \n",
    "eval_image = np.ndarray([eval_num,image_WH,image_WH,3])\n",
    "eval_label = np.ndarray([eval_num])\n",
    "\n",
    "i = 0\n",
    "for line in lines:\n",
    "    if i == image_num: break\n",
    "    eval_label[i] = np.asanyarray(int(line[-3:-1])).reshape([1])\n",
    "    eval_image[i] = np.asanyarray(Image.open(line[:-4])).reshape((image_WH,image_WH,3))\n",
    "    i += 1\n",
    "print(\"eval image : \"+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "program is started\n",
      "C:\\Users\\HSJung\\jupyterNotebook\\01_Wom_Classification\\vgg16\\vgg16.npy\n",
      "npy file loaded\n",
      "build model started\n",
      "build model finished: 12s\n",
      "2018-09-27 17:10:11.264331  Running starts!\n",
      "[ 2018-09-27 17:12:00.796970  1 epoch] loss : 19246.0511 train accuracy : 0.444751 validation accuracy : 0.535714\n",
      "[ 2018-09-27 17:13:53.123553  2 epoch] loss : 3354.0262 train accuracy : 0.780387 validation accuracy : 0.576531\n",
      "[ 2018-09-27 17:15:32.769561  3 epoch] loss : 2398.1233 train accuracy : 0.837017 validation accuracy : 0.637755\n",
      "[ 2018-09-27 17:17:12.261688  4 epoch] loss : 2054.2125 train accuracy : 0.886740 validation accuracy : 0.647959\n",
      "[ 2018-09-27 17:18:51.892890  5 epoch] loss : 1207.4231 train accuracy : 0.907459 validation accuracy : 0.668367\n",
      "[ 2018-09-27 17:20:32.398794  6 epoch] loss : 1182.8112 train accuracy : 0.915746 validation accuracy : 0.673469\n",
      "[ 2018-09-27 17:22:12.355544  7 epoch] loss : 576.4886 train accuracy : 0.953039 validation accuracy : 0.673469\n",
      "[ 2018-09-27 17:23:51.614931  8 epoch] loss : 930.1114 train accuracy : 0.951657 validation accuracy : 0.709184\n",
      "[ 2018-09-27 17:25:31.739441  9 epoch] loss : 778.0574 train accuracy : 0.954420 validation accuracy : 0.663265\n",
      "[ 2018-09-27 17:27:11.338482 10 epoch] loss : 452.3375 train accuracy : 0.961326 validation accuracy : 0.709184\n",
      "[ 2018-09-27 17:28:50.883573 11 epoch] loss : 928.3274 train accuracy : 0.936464 validation accuracy : 0.596939\n",
      "[ 2018-09-27 17:30:42.254474 12 epoch] loss : 1054.7492 train accuracy : 0.953039 validation accuracy : 0.693878\n",
      "[ 2018-09-27 17:32:21.465904 13 epoch] loss : 419.8226 train accuracy : 0.972376 validation accuracy : 0.683673\n",
      "[ 2018-09-27 17:34:00.684320 14 epoch] loss : 832.6118 train accuracy : 0.959945 validation accuracy : 0.688776\n",
      "[ 2018-09-27 17:35:40.261362 15 epoch] loss : 505.2188 train accuracy : 0.977901 validation accuracy : 0.704082\n",
      "[ 2018-09-27 17:37:19.879821 16 epoch] loss : 381.2065 train accuracy : 0.980663 validation accuracy : 0.704082\n",
      "[ 2018-09-27 17:38:59.307003 17 epoch] loss : 183.6250 train accuracy : 0.984807 validation accuracy : 0.663265\n",
      "[ 2018-09-27 17:40:38.665759 18 epoch] loss : 544.3151 train accuracy : 0.964088 validation accuracy : 0.698980\n",
      "[ 2018-09-27 17:42:18.311732 19 epoch] loss : 1205.4439 train accuracy : 0.957182 validation accuracy : 0.576531\n",
      "[ 2018-09-27 17:43:57.787890 20 epoch] loss : 1791.4270 train accuracy : 0.936464 validation accuracy : 0.632653\n",
      "[ 2018-09-27 17:45:37.434858 21 epoch] loss : 1329.2381 train accuracy : 0.962707 validation accuracy : 0.688776\n",
      "[ 2018-09-27 17:47:28.114400 22 epoch] loss : 1608.4857 train accuracy : 0.959945 validation accuracy : 0.693878\n",
      "[ 2018-09-27 17:49:07.809533 23 epoch] loss : 1579.9879 train accuracy : 0.940608 validation accuracy : 0.734694\n",
      "[ 2018-09-27 17:50:47.330631 24 epoch] loss : 704.8702 train accuracy : 0.958564 validation accuracy : 0.734694\n",
      "[ 2018-09-27 17:52:27.406525 25 epoch] loss : 290.0416 train accuracy : 0.983425 validation accuracy : 0.714286\n",
      "[ 2018-09-27 17:54:08.469886 26 epoch] loss : 170.1587 train accuracy : 0.984807 validation accuracy : 0.724490\n",
      "[ 2018-09-27 17:55:50.965754 27 epoch] loss : 379.7396 train accuracy : 0.986188 validation accuracy : 0.724490\n",
      "[ 2018-09-27 17:57:31.665970 28 epoch] loss : 180.1350 train accuracy : 0.993094 validation accuracy : 0.709184\n",
      "[ 2018-09-27 17:59:13.404959 29 epoch] loss : 434.3223 train accuracy : 0.993094 validation accuracy : 0.698980\n",
      "[ 2018-09-27 18:00:56.110664 30 epoch] loss : 97.5944 train accuracy : 0.994475 validation accuracy : 0.719388\n",
      "[ 2018-09-27 18:02:37.356830 31 epoch] loss : 254.9901 train accuracy : 0.986188 validation accuracy : 0.709184\n",
      "[ 2018-09-27 18:04:30.532401 32 epoch] loss : 381.3231 train accuracy : 0.980663 validation accuracy : 0.642857\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as g:\n",
    "    with tf.Session(config=tf.ConfigProto(gpu_options=(tf.GPUOptions(per_process_gpu_memory_fraction=0.6)))) as sess:\n",
    "        '''\n",
    "        #Load meta graph\n",
    "        saver = tf.train.import_meta_graph(os.path.join(ckpt_loc,'TL-model-1280.meta'))\n",
    "        \n",
    "        #Load weights\n",
    "        saver.restore(sess, tf.train.latest_checkpoint(ckpt_loc))\n",
    "        #make graph\n",
    "        graph = tf.get_default_graph()\n",
    "        \n",
    "        # If you don't know name of variables, remove '''''' \n",
    "        \"\"\"\n",
    "        \n",
    "        for op in graph.get_operations():\n",
    "            print(op.name)\n",
    "        \"\"\"\n",
    "        \n",
    "        #Fix input and ouput\n",
    "        inputs = graph.get_tensor_by_name(\"images:0\")\n",
    "        labels = graph.get_tensor_by_name(\"labels:0\")\n",
    "        prob = graph.get_tensor_by_name(\"content_vgg/fc8/probability:0\")\n",
    "        \n",
    "        #For saving checkpoint\n",
    "        ckpt = tf.train.get_checkpoint_state(ckpt_loc)\n",
    "        '''\n",
    "        \n",
    "        #variables for input and ground truth\n",
    "        print('program is started')\n",
    "        images = tf.placeholder(\"float32\", [None, image_WH, image_WH, 3],name = 'images')\n",
    "        labels = tf.placeholder(\"int64\",[None], name = 'labels')\n",
    "\n",
    "        vgg = vgg16_v1.Vgg16()\n",
    " \n",
    "        with tf.name_scope(\"content_vgg\"):\n",
    "            vgg.build(images,labels)\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            filter_img = tf.summary.FileWriter('filter',sess.graph)\n",
    "            saver = tf.train.Saver()\n",
    "            \n",
    "            start = datetime.now()\n",
    "            print(datetime.now(),' Running starts!')\n",
    "            \n",
    "            \n",
    "            for epoch in range(epoch_num):\n",
    "                img_count = 0\n",
    "                loss_sum = 0.0\n",
    "                acc_sum_t = 0.0 \n",
    "                acc_sum_e = 0.0\n",
    "                eval_count = 0\n",
    "                \n",
    "                for batch in range(batch_num_t):\n",
    "                    feed_dict_train = {images: image[batch*batch_size:(batch+1)*batch_size], labels: label[batch*batch_size:(batch+1)*batch_size]}\n",
    "                    \n",
    "                    loss, train_acc, _ = sess.run([vgg.cost, vgg.accuracy, vgg.train], feed_dict=feed_dict_train)\n",
    "                    \n",
    "                    img_count += batch_size\n",
    "                    \n",
    "                    \n",
    "                    loss_sum +=  loss\n",
    "                    acc_sum_t += train_acc\n",
    "                \n",
    "                for batch in range(batch_num_e):\n",
    "                    feed_dict_eval = {images: eval_image[batch*batch_size:(batch+1)*batch_size], labels: eval_label[batch*batch_size:(batch+1)*batch_size]}\n",
    "                    eval_acc= sess.run(vgg.accuracy, feed_dict=feed_dict_eval)\n",
    "\n",
    "                    acc_sum_e += eval_acc\n",
    "                \n",
    "                loss_[epoch] = np.sum(loss_sum)/batch_num_t\n",
    "                acc_t = np.sum(acc_sum_t)/batch_num_t\n",
    "                acc_e = np.sum(acc_sum_e)/batch_num_e\n",
    "                global_step = image_num*(epoch+1)\n",
    "                \n",
    "                print('[',datetime.now(),\"%2d\" %(epoch+1),\"epoch] loss : %7.4f\" % loss_[epoch], \"train accuracy : %.6f\" % acc_t, \"validation accuracy : %.6f\" % acc_e)\n",
    "                \n",
    "                if epoch % 10 == 0:\n",
    "                    saver.save(sess,os.path.join(ckpt_loc,\"TL-model\"),global_step = global_step) #write_meta_graph=False meta 원하지 않음\n",
    "                    if acc_t == 1.0:\n",
    "                        break\n",
    "            print(datetime.now(),' Running finished!')\n",
    "            print('-img.close')\n",
    "            filter_img.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(loss_[0])\n",
    "print(loss_[1])\n",
    "x = loss_[0:3]\n",
    "plt.plot(x)\n",
    "plt.title(\"Training Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
